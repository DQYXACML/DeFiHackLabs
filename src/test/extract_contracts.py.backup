#!/usr/bin/env python3
"""
DeFiæ”»å‡»åˆçº¦æºç æå–å·¥å…·

åŠŸèƒ½:
1. é™æ€åˆ†æ: ä»æ”»å‡»è„šæœ¬ä¸­æå–æ˜¾å¼å®šä¹‰çš„åˆçº¦åœ°å€
2. åŠ¨æ€åˆ†æ: è¿è¡Œforge testè·å–å®Œæ•´çš„åˆçº¦è°ƒç”¨é“¾
3. æºç ä¸‹è½½: ä»åŒºå—æµè§ˆå™¨APIä¸‹è½½å·²éªŒè¯çš„åˆçº¦æºç 

ä½œè€…: Claude Code
ç‰ˆæœ¬: 1.0.0
"""

import re
import json
import os
import subprocess
import time
import requests
from pathlib import Path
from typing import Any, List, Dict, Set, Optional, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict
import logging
import tempfile
import shutil
from contextlib import contextmanager
import threading
from queue import Queue, Empty
from concurrent.futures import ThreadPoolExecutor, as_completed

# ============================================================================
# é…ç½®
# ============================================================================

# API Keysé…ç½® (å†™æ­»åœ¨è„šæœ¬ä¸­)
# æ³¨æ„: Etherscan API V2ç»Ÿä¸€æ”¯æŒå¤šä¸ªç½‘ç»œ,åŒ…æ‹¬BSC!
# æ”¯æŒå¤šä¸ªkeyå¹¶å‘,æ¯ä¸ªkeyé™é€Ÿ5æ¬¡/ç§’
DEFAULT_API_KEYS = {
    "etherscan": [
        "2DTB79CHTEJ6PEDCTEINC8GV3IHUXHGP9A",
        "NNBK8BWF9FCBY77Y2C1S5GG5CACNJIAQ8C",
        "K6RUIHP3NJ72D4F3MNVG8XMI6R8EE1JSJD",
        "SMZQJGY9IVWYKUMK2SIME6F15HGD8F8I6C"
    ],
    # å…¶ä»–ç‹¬ç«‹ç½‘ç»œçš„Keyå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ (ä¹Ÿæ”¯æŒåˆ—è¡¨):
    # "arbiscan": ["YOUR_ARBISCAN_KEY_1", "YOUR_ARBISCAN_KEY_2"],
    # "polygonscan": ["YOUR_POLYGONSCAN_KEY"],
}

# åŒºå—æµè§ˆå™¨APIé…ç½® (V2)
# æ³¨æ„: Etherscanå·²è¿ç§»åˆ°V2 API,éœ€è¦chainidå‚æ•°
EXPLORER_APIS = {
    "mainnet": {
        "name": "Etherscan",
        "api_url": "https://api.etherscan.io/v2/api",
        "web_url": "https://etherscan.io",
        "chainid": 1,
        "api_key_name": "etherscan",
        "rpc_url": "https://lb.drpc.live/ethereum/Avduh2iIjEAksBUYtd4wP1NUPObEnwYR76WEFhW5UfFk"
    },
    "arbitrum": {
        "name": "Arbiscan",
        "api_url": "https://api.etherscan.io/v2/api",
        "web_url": "https://arbiscan.io",
        "chainid": 42161,
        "api_key_name": "etherscan",
        "rpc_url": "https://lb.drpc.live/arbitrum/Avduh2iIjEAksBUYtd4wP1NUPObEnwYR76WEFhW5UfFk"
    },
    "bsc": {
        "name": "BscScan",
        "api_url": "https://api.etherscan.io/v2/api",
        "web_url": "https://bscscan.com",
        "chainid": 56,
        "api_key_name": "etherscan",
        "rpc_url": "https://frequent-magical-tree.bsc.quiknode.pro/366dcd223ff09bb824b35ca8d190632061cdf022/"
    },
    "base": {
        "name": "BaseScan",
        "api_url": "https://api.basescan.org/v2/api",
        "web_url": "https://basescan.org",
        "chainid": 8453,
        "api_key_name": "etherscan",
        "rpc_url": "https://lb.drpc.live/base/Avduh2iIjEAksBUYtd4wP1NUPObEnwYR76WEFhW5UfFk"
    },
    "optimism": {
        "name": "Optimism Etherscan",
        "api_url": "https://api-optimistic.etherscan.io/v2/api",
        "web_url": "https://optimistic.etherscan.io",
        "chainid": 10,
        "api_key_name": "etherscan",
        "rpc_url": "https://lb.drpc.live/optimism/Avduh2iIjEAksBUYtd4wP1NUPObEnwYR76WEFhW5UfFk"
    },
    "blast": {
        "name": "BlastScan",
        "api_url": "https://api.blastscan.io/v2/api",
        "web_url": "https://blastscan.io",
        "chainid": 81457,
        "api_key_name": "etherscan",
        "rpc_url": "https://lb.drpc.live/blast/Avduh2iIjEAksBUYtd4wP1NUPObEnwYR76WEFhW5UfFk"
    },
    "polygon": {
        "name": "PolygonScan",
        "api_url": "https://api.polygonscan.com/v2/api",
        "web_url": "https://polygonscan.com",
        "chainid": 137,
        "api_key_name": "polygonscan",
        "rpc_url": "https://lb.drpc.live/polygon/Avduh2iIjEAksBUYtd4wP1NUPObEnwYR76WEFhW5UfFk"
    },
    "avalanche": {
        "name": "SnowTrace",
        "api_url": "https://api.snowtrace.io/v2/api",
        "web_url": "https://snowtrace.io",
        "chainid": 43114,
        "api_key_name": "snowtrace",
        "rpc_url": "https://lb.drpc.live/avalanche/Avduh2iIjEAksBUYtd4wP1NUPObEnwYR76WEFhW5UfFk"
    },
    "fantom": {
        "name": "FTMScan",
        "api_url": "https://api.ftmscan.com/v2/api",
        "web_url": "https://ftmscan.com",
        "chainid": 250,
        "api_key_name": "ftmscan",
        "rpc_url": "https://lb.drpc.live/fantom/Avduh2iIjEAksBUYtd4wP1NUPObEnwYR76WEFhW5UfFk"
    },
}

# forge æµ‹è¯•é»˜è®¤è·³è¿‡çš„è„šæœ¬ï¼ˆå·²çŸ¥ç¼–è¯‘é—®é¢˜ï¼‰
DEFAULT_SKIP_TESTS = [
    "src/test/2025-05/Corkprotocol_exp.sol",
    "src/test/2024-11/proxy_b7e1_exp.sol",
]

# å…è´¹APIé™æµé…ç½®
API_RATE_LIMIT = 5  # 5æ¬¡/ç§’
API_RETRY_TIMES = 3
API_RETRY_DELAY = 2  # ç§’

# æ—¥å¿—é…ç½®
LOG_FORMAT = '%(asctime)s - %(levelname)s - %(message)s'
logging.basicConfig(level=logging.INFO, format=LOG_FORMAT)
logger = logging.getLogger(__name__)

# ============================================================================
# è·¯å¾„é…ç½®
# ============================================================================

SCRIPT_DIR = Path(__file__).resolve().parent
try:
    PROJECT_ROOT = SCRIPT_DIR.parents[1]
except IndexError:
    PROJECT_ROOT = SCRIPT_DIR
DEFAULT_TEST_DIR = PROJECT_ROOT / 'src/test'
DEFAULT_OUTPUT_DIR = PROJECT_ROOT / 'extracted_contracts'

# ============================================================================
# æ•°æ®ç»“æ„
# ============================================================================

@dataclass
class ContractAddress:
    """åˆçº¦åœ°å€ä¿¡æ¯"""
    address: str
    name: Optional[str] = None
    chain: Optional[str] = None
    source: str = "unknown"  # static/dynamic/comment
    context: Optional[str] = None  # æå–ä¸Šä¸‹æ–‡

    def __hash__(self):
        return hash(self.address.lower())

    def __eq__(self, other):
        if isinstance(other, ContractAddress):
            return self.address.lower() == other.address.lower()
        return False


@dataclass
class ExploitScript:
    """æ”»å‡»è„šæœ¬ä¿¡æ¯"""
    file_path: Path
    name: str
    date_dir: str
    chain: Optional[str] = None
    block_number: Optional[int] = None
    loss_amount: Optional[str] = None
    attack_tx: Optional[str] = None


@dataclass
class ExecutionSummary:
    """æ‰§è¡Œæ‘˜è¦"""
    total_scripts: int = 0
    successful_scripts: int = 0
    failed_scripts: int = 0
    total_addresses: int = 0
    verified_contracts: int = 0
    unverified_contracts: int = 0
    bytecode_only_contracts: int = 0  # ä»…ä¸‹è½½å­—èŠ‚ç çš„åˆçº¦æ•°
    api_calls: int = 0
    errors: List[str] = None

    def __post_init__(self):
        if self.errors is None:
            self.errors = []


# ============================================================================
# é™æ€åˆ†ææ¨¡å—
# ============================================================================

class StaticAnalyzer:
    """é™æ€åˆ†æå™¨ - ä»æºç ä¸­æå–åˆçº¦åœ°å€"""

    # ä»¥å¤ªåŠåœ°å€æ­£åˆ™
    ETH_ADDRESS_PATTERN = re.compile(r'0x[a-fA-F0-9]{40}')

    # å¸¸è§çš„åœ°å€å®šä¹‰æ¨¡å¼
    ADDRESS_PATTERNS = [
        # address constant NAME = 0x...
        re.compile(r'address\s+(?:constant\s+)?(?:public\s+)?(\w+)\s*=\s*(0x[a-fA-F0-9]{40})'),
        # IERC20 constant token = IERC20(0x...)
        re.compile(r'(\w+)\s+constant\s+(\w+)\s*=\s*\w+\((0x[a-fA-F0-9]{40})\)'),
        # Interface(0x...)
        re.compile(r'\w+\((0x[a-fA-F0-9]{40})\)'),
    ]

    # æ³¨é‡Šä¸­çš„å…³é”®å­—
    COMMENT_KEYWORDS = [
        'Attacker',
        'Attack Contract',
        'Vulnerable Contract',
        'Victim'
    ]

    def __init__(self):
        self.logger = logging.getLogger(__name__ + '.StaticAnalyzer')

    def analyze_script(self, script: ExploitScript) -> Tuple[List[ContractAddress], str]:
        """
        åˆ†æå•ä¸ªè„šæœ¬

        Returns:
            (åœ°å€åˆ—è¡¨, é“¾ç±»å‹)
        """
        self.logger.info(f"é™æ€åˆ†æ: {script.name}")

        try:
            with open(script.file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            self.logger.error(f"è¯»å–æ–‡ä»¶å¤±è´¥ {script.file_path}: {e}")
            return [], None

        addresses = []

        # 1. ä»æ³¨é‡Šä¸­æå–åœ°å€
        addresses.extend(self._extract_from_comments(content))

        # 2. ä»ä»£ç ä¸­æå–å¸¸é‡å®šä¹‰
        addresses.extend(self._extract_from_code(content))

        # 3. æå–é“¾ç±»å‹
        chain = self._extract_chain(content)

        # 4. æå–åŒºå—å·
        block_number = self._extract_block_number(content)
        script.block_number = block_number
        script.chain = chain

        # å»é‡
        unique_addresses = list(dict.fromkeys(addresses))

        self.logger.info(f"  é™æ€æå–åˆ° {len(unique_addresses)} ä¸ªåœ°å€")
        return unique_addresses, chain

    def _extract_from_comments(self, content: str) -> List[ContractAddress]:
        """ä»æ³¨é‡Šä¸­æå–åœ°å€"""
        addresses = []
        lines = content.split('\n')

        for line in lines:
            if not line.strip().startswith('//'):
                continue

            # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®å­—
            for keyword in self.COMMENT_KEYWORDS:
                if keyword in line:
                    # æå–URLå’Œåœ°å€
                    urls = re.findall(r'https://[^\s]+', line)
                    for url in urls:
                        # ä»URLä¸­æå–åœ°å€
                        match = self.ETH_ADDRESS_PATTERN.search(url)
                        if match:
                            addr = match.group(0)
                            chain = self._chain_from_url(url)
                            addresses.append(ContractAddress(
                                address=addr,
                                name=keyword.replace(' ', '_'),
                                chain=chain,
                                source='comment',
                                context=line.strip()
                            ))

                    # ç›´æ¥ä»æ³¨é‡Šä¸­æå–åœ°å€
                    if not urls:
                        match = self.ETH_ADDRESS_PATTERN.search(line)
                        if match:
                            addr = match.group(0)
                            addresses.append(ContractAddress(
                                address=addr,
                                name=keyword.replace(' ', '_'),
                                source='comment',
                                context=line.strip()
                            ))

        return addresses

    def _extract_from_code(self, content: str) -> List[ContractAddress]:
        """ä»ä»£ç ä¸­æå–åœ°å€"""
        addresses = []

        # æå–address constantå®šä¹‰
        for pattern in self.ADDRESS_PATTERNS:
            matches = pattern.finditer(content)
            for match in matches:
                groups = match.groups()
                # æŸ¥æ‰¾åœ°å€
                addr = None
                name = None
                for g in groups:
                    if g and g.startswith('0x') and len(g) == 42:
                        addr = g
                    elif g and not g.startswith('0x'):
                        name = g

                if addr:
                    addresses.append(ContractAddress(
                        address=addr,
                        name=name,
                        source='static',
                        context=match.group(0)
                    ))

        return addresses

    def _extract_chain(self, content: str) -> Optional[str]:
        """æå–é“¾ç±»å‹"""
        match = re.search(r'createSelectFork\("(\w+)"', content)
        if match:
            return match.group(1)
        return None

    def _extract_block_number(self, content: str) -> Optional[int]:
        """æå–åŒºå—å·"""
        match = re.search(r'blocknumToForkFrom\s*=\s*(\d+)', content)
        if match:
            return int(match.group(1))
        return None

    def _chain_from_url(self, url: str) -> Optional[str]:
        """ä»URLè¯†åˆ«é“¾ç±»å‹"""
        if 'etherscan.io' in url:
            return 'mainnet'
        elif 'bscscan.com' in url:
            return 'bsc'
        elif 'arbiscan.io' in url:
            return 'arbitrum'
        elif 'basescan.org' in url:
            return 'base'
        return None


# ============================================================================
# åŠ¨æ€åˆ†ææ¨¡å—
# ============================================================================

class DynamicAnalyzer:
    """åŠ¨æ€åˆ†æå™¨ - è¿è¡Œforge testå¹¶è§£ætrace"""

    # åŒ¹é…CALLæŒ‡ä»¤çš„åœ°å€ - æ ¼å¼: [gas] address::function(...)
    CALL_PATTERN = re.compile(r'\[(\d+)\]\s+(0x[a-fA-F0-9]{40})::\w+')
    # åŒ¹é…åˆçº¦åˆ›å»º - æ ¼å¼: â†’ new ContractName@address æˆ– â†’ address
    CREATE_PATTERN = re.compile(r'â†’\s+(?:new\s+\w+@)?(0x[a-fA-F0-9]{40})')
    # åŒ¹é…traceæ®µè½ä¸­çš„ä»»æ„åœ°å€(ç”¨äºå…œåº•æ•è·)
    HEX_ADDRESS_PATTERN = re.compile(r'0x[a-fA-F0-9]{40}')
    TRACES_SECTION_PATTERN = re.compile(r'Traces:\s*(.+)', re.S)

    def __init__(self, skip_tests: Optional[List[str]] = None):
        self.logger = logging.getLogger(__name__ + '.DynamicAnalyzer')
        self.project_root = PROJECT_ROOT
        self.skip_tests = skip_tests or DEFAULT_SKIP_TESTS

    def analyze_script(self, script: ExploitScript) -> List[ContractAddress]:
        """
        è¿è¡Œforge testå¹¶æå–è°ƒç”¨çš„æ‰€æœ‰åˆçº¦

        Returns:
            åœ°å€åˆ—è¡¨
        """
        self.logger.info(f"åŠ¨æ€åˆ†æ: {script.name}")

        try:
            # è¿è¡Œforge test (é»˜è®¤ -vvvv)
            output = self._run_forge_test(script.file_path)

            if output is None:
                self.logger.warning(f"  æµ‹è¯•è¿è¡Œå¤±è´¥,è·³è¿‡åŠ¨æ€åˆ†æ")
                return []

            addresses = self._parse_trace(output)
            if not addresses:
                # è‡ªåŠ¨å°è¯•ä½¿ç”¨ -vvvvv ä»¥è¾“å‡ºæ›´å®Œæ•´çš„è°ƒç”¨æ ˆ
                self.logger.info("  æœªè§£æåˆ°traceï¼Œä½¿ç”¨ -vvvvv é‡æ–°è¿è¡Œä»¥å¼ºåˆ¶è¾“å‡º")
                trace_output = self._run_forge_test(script.file_path, extra_flags=['-vvvvv'])
                if trace_output:
                    addresses = self._parse_trace(trace_output)
                    output = trace_output  # ä¾¿äºåç»­åˆ¤æ–­æ—¥å¿—çŠ¶æ€

            if not addresses and 'Traces:' not in output:
                self.logger.warning("  forge è¾“å‡ºä¸­ä»æœªåŒ…å« Tracesï¼Œå¯è€ƒè™‘åœ¨æµ‹è¯•ä¸­åŠ å…¥ emit/log è§¦å‘ trace")

            self.logger.info(f"  åŠ¨æ€æå–åˆ° {len(addresses)} ä¸ªåœ°å€")
            return addresses

        except Exception as e:
            self.logger.error(f"åŠ¨æ€åˆ†æå¤±è´¥: {e}")
            return []

    def _run_forge_test(self, test_file: Path, timeout: int = 300,
                        extra_flags: Optional[List[str]] = None) -> Optional[str]:
        """
        è¿è¡Œforge test

        Args:
            test_file: æµ‹è¯•æ–‡ä»¶è·¯å¾„
            timeout: è¶…æ—¶æ—¶é—´(ç§’)

        Returns:
            æµ‹è¯•è¾“å‡ºæˆ–None(å¦‚æœå¤±è´¥)
        """
        try:
            try:
                match_path = test_file.relative_to(self.project_root)
            except ValueError:
                self.logger.error(f"  æµ‹è¯•æ–‡ä»¶ä¸åœ¨é¡¹ç›®æ ¹ç›®å½•å†…: {test_file}")
                return None

            flags = extra_flags if extra_flags is not None else ['-vvvv']
            cmd = [
                'forge', 'test',
                '--match-path', str(match_path)
            ]

            # è·³è¿‡å·²çŸ¥å­˜åœ¨ç¼–è¯‘/æ‰§è¡Œé—®é¢˜çš„æµ‹è¯•è„šæœ¬ï¼Œé¿å…é˜»å¡å½“å‰åˆ†æ
            for skip in self.skip_tests:
                if skip:
                    cmd.extend(['--skip', skip])

            cmd.extend(flags)

            working_dir = self.project_root
            self.logger.debug(f"  æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            self.logger.debug(f"  å·¥ä½œç›®å½•: {working_dir}")

            result = subprocess.run(
                cmd,
                cwd=working_dir,
                capture_output=True,
                text=True,
                timeout=timeout
            )

            # å³ä½¿æµ‹è¯•å¤±è´¥,ä¹Ÿå¯èƒ½æœ‰traceè¾“å‡º
            output = result.stdout + result.stderr

            if result.returncode != 0:
                self.logger.warning(f"  æµ‹è¯•è¿”å›éé›¶çŠ¶æ€ç : {result.returncode}")
                trimmed_output = output.strip()
                if trimmed_output:
                    lines = trimmed_output.splitlines()
                    max_lines = 120
                    if len(lines) > max_lines:
                        trimmed_output = "\n".join(lines[-max_lines:])
                        self.logger.warning(f"  forge è¾“å‡º(æœ«å°¾ {max_lines} è¡Œ):\n{trimmed_output}")
                    else:
                        self.logger.warning(f"  forge è¾“å‡º:\n{trimmed_output}")
                # æ£€æŸ¥æ˜¯å¦æœ‰traceè¾“å‡º
                if 'Traces:' not in output:
                    return None

            return output

        except subprocess.TimeoutExpired:
            self.logger.error(f"  æµ‹è¯•è¶…æ—¶({timeout}ç§’)")
            return None
        except FileNotFoundError:
            self.logger.error("  forgeå‘½ä»¤æœªæ‰¾åˆ°,è¯·ç¡®ä¿å·²å®‰è£…Foundry")
            return None
        except Exception as e:
            self.logger.error(f"  è¿è¡Œforge testå¤±è´¥: {e}")
            return None

    @contextmanager
    def _isolated_project(self, test_file: Path) -> Tuple[Path, Path]:
        """ä¸ºæŒ‡å®šæµ‹è¯•è„šæœ¬åˆ›å»ºéš”ç¦»çš„Foundryå·¥ç¨‹ç¯å¢ƒ"""
        temp_dir = Path(tempfile.mkdtemp(prefix="extractor_"))
        try:
            self._copy_project_configs(temp_dir)

            files_to_copy = self._collect_dependencies(test_file)
            for src in files_to_copy:
                if not src.exists():
                    continue
                try:
                    rel_path = src.relative_to(self.project_root)
                except ValueError:
                    continue
                dest = temp_dir / rel_path
                dest.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(src, dest)

            try:
                relative_test = test_file.relative_to(self.project_root)
            except ValueError:
                raise RuntimeError(f"æµ‹è¯•æ–‡ä»¶ä¸åœ¨é¡¹ç›®æ ¹ç›®å½•å†…: {test_file}")

            match_path = (temp_dir / relative_test).resolve()
            yield temp_dir, match_path.relative_to(temp_dir)
        finally:
            shutil.rmtree(temp_dir, ignore_errors=True)

    def _copy_project_configs(self, temp_dir: Path):
        """å¤åˆ¶Foundryé…ç½®æ–‡ä»¶ä¸ä¾èµ–åº“"""
        for name in ('foundry.toml', 'remappings.txt'):
            src = self.project_root / name
            if src.exists():
                shutil.copy2(src, temp_dir / name)

        lib_src = self.project_root / 'lib'
        lib_dest = temp_dir / 'lib'
        if lib_src.exists() and not lib_dest.exists():
            try:
                os.symlink(lib_src, lib_dest, target_is_directory=True)
            except OSError:
                shutil.copytree(lib_src, lib_dest)

    IMPORT_PATTERN = re.compile(r'import\s+(?:\{[^}]*\}\s+from\s+)?["\']([^"\']+)["\'];?')

    def _collect_dependencies(self, test_file: Path) -> Set[Path]:
        """é€’å½’æ”¶é›†æµ‹è¯•è„šæœ¬çš„ç›¸å¯¹ä¾èµ–"""
        to_process = [test_file.resolve()]
        collected: Set[Path] = set()

        while to_process:
            current = to_process.pop()
            if current in collected:
                continue
            collected.add(current)

            try:
                content = current.read_text()
            except Exception:
                continue

            for match in self.IMPORT_PATTERN.finditer(content):
                import_path = match.group(1)
                dependency = self._resolve_import_path(current, import_path)
                if dependency and dependency.suffix == '.sol' and dependency.exists():
                    to_process.append(dependency.resolve())

        return collected

    def _resolve_import_path(self, current_file: Path, import_path: str) -> Optional[Path]:
        """è§£æå¯¼å…¥è·¯å¾„,æ”¯æŒç›¸å¯¹è·¯å¾„ä»¥åŠsrc/test/scriptå‰ç¼€"""
        if import_path.startswith('.'):
            return (current_file.parent / import_path).resolve()
        if import_path.startswith('src/'):
            return (self.project_root / import_path).resolve()
        if import_path.startswith('test/'):
            return (self.project_root / import_path).resolve()
        if import_path.startswith('script/'):
            return (self.project_root / import_path).resolve()
        return None

    def _parse_trace(self, output: str) -> List[ContractAddress]:
        """è§£ætraceè¾“å‡ºæå–åˆçº¦åœ°å€"""
        addresses = []
        seen = set()

        # æå–CALLè°ƒç”¨çš„åˆçº¦
        for match in self.CALL_PATTERN.finditer(output):
            gas = match.group(1)
            address = match.group(2)

            if address.lower() not in seen:
                seen.add(address.lower())
                addresses.append(ContractAddress(
                    address=address,
                    source='dynamic',
                    context=f'called with gas {gas}'
                ))

        # æå–CREATEåˆ›å»ºçš„åˆçº¦
        for match in self.CREATE_PATTERN.finditer(output):
            address = match.group(1)
            if address.lower() not in seen:
                seen.add(address.lower())
                addresses.append(ContractAddress(
                    address=address,
                    source='dynamic',
                    context='contract created'
                ))

        # é¢å¤–è§£æTracesæ®µè½ä¸­å‡ºç°çš„æ‰€æœ‰åœ°å€
        trace_body = self._extract_traces_body(output)
        if trace_body:
            for match in self.HEX_ADDRESS_PATTERN.finditer(trace_body):
                address = match.group(0)
                lowered = address.lower()
                if lowered in seen:
                    continue
                seen.add(lowered)
                addresses.append(ContractAddress(
                    address=address,
                    source='dynamic',
                    context='trace reference'
                ))

        return addresses

    def _extract_traces_body(self, output: str) -> Optional[str]:
        """æå– forge è¾“å‡ºä¸­çš„ Traces æ®µè½"""
        match = self.TRACES_SECTION_PATTERN.search(output)
        if not match:
            return None
        return match.group(1)


# ============================================================================
# API Keyæ± ç®¡ç†æ¨¡å— (æ”¯æŒå¤šKeyå¹¶å‘)
# ============================================================================

class APIKeyPool:
    """
    API Keyæ± ç®¡ç†å™¨ - æ”¯æŒå¤šä¸ªkeyå¹¶å‘è¯·æ±‚,æ¯ä¸ªkeyç‹¬ç«‹é™æµ

    ç‰¹æ€§:
    - å¤šä¸ªAPI Keyå¹¶å‘ä½¿ç”¨
    - æ¯ä¸ªKeyç‹¬ç«‹é™æµ(é»˜è®¤5æ¬¡/ç§’)
    - çº¿ç¨‹å®‰å…¨çš„Keyè·å–/å½’è¿˜
    - é¿å…æ­»é”å’Œèµ„æºå†²çª
    """

    def __init__(self, keys: List[str], rate_limit: int = 5):
        """
        åˆå§‹åŒ–API Keyæ± 

        Args:
            keys: API Keyåˆ—è¡¨
            rate_limit: æ¯ä¸ªkeyçš„é™é€Ÿ(æ¬¡/ç§’)
        """
        self.keys = keys
        self.rate_limit = rate_limit
        self.logger = logging.getLogger(__name__ + '.APIKeyPool')

        # ä¸ºæ¯ä¸ªkeyåˆ›å»ºç‹¬ç«‹çš„é™æµçŠ¶æ€
        self.key_states = {
            key: {
                'last_call_time': 0.0,
                'lock': threading.Lock(),
                'call_count': 0
            }
            for key in keys
        }

        # ä½¿ç”¨Queueç®¡ç†å¯ç”¨çš„key
        self.available_keys = Queue()
        for key in keys:
            self.available_keys.put(key)

        # ç»Ÿè®¡ä¿¡æ¯
        self.total_calls = 0
        self.stats_lock = threading.Lock()

        self.logger.info(f"åˆå§‹åŒ–API Keyæ± : {len(keys)} ä¸ªkey, é™é€Ÿ {rate_limit} æ¬¡/ç§’")

    @contextmanager
    def acquire_key(self, timeout: float = 30.0):
        """
        è·å–ä¸€ä¸ªå¯ç”¨çš„API Key (ä¸Šä¸‹æ–‡ç®¡ç†å™¨)

        Args:
            timeout: è·å–keyçš„è¶…æ—¶æ—¶é—´(ç§’)

        Yields:
            API Keyå­—ç¬¦ä¸²

        ä½¿ç”¨ç¤ºä¾‹:
            with key_pool.acquire_key() as api_key:
                # ä½¿ç”¨api_keyå‘èµ·è¯·æ±‚
                response = requests.get(url, params={'apikey': api_key})
        """
        key = None
        try:
            # ä»é˜Ÿåˆ—ä¸­è·å–ä¸€ä¸ªå¯ç”¨çš„key
            try:
                key = self.available_keys.get(timeout=timeout)
            except Empty:
                raise RuntimeError(f"è·å–API Keyè¶…æ—¶({timeout}ç§’),æ‰€æœ‰keyéƒ½åœ¨ä½¿ç”¨ä¸­")

            # åº”ç”¨è¯¥keyçš„é™æµ
            self._apply_rate_limit(key)

            # è¿”å›keyä¾›ä½¿ç”¨
            yield key

        finally:
            # ç¡®ä¿keyè¢«å½’è¿˜åˆ°é˜Ÿåˆ—
            if key is not None:
                self.available_keys.put(key)

    def _apply_rate_limit(self, key: str):
        """å¯¹æŒ‡å®škeyåº”ç”¨é™æµ"""
        state = self.key_states[key]

        with state['lock']:
            now = time.time()
            elapsed = now - state['last_call_time']

            # è®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´
            min_interval = 1.0 / self.rate_limit
            if elapsed < min_interval:
                sleep_time = min_interval - elapsed
                time.sleep(sleep_time)

            # æ›´æ–°çŠ¶æ€
            state['last_call_time'] = time.time()
            state['call_count'] += 1

            # æ›´æ–°æ€»è°ƒç”¨æ¬¡æ•°
            with self.stats_lock:
                self.total_calls += 1

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self.stats_lock:
            key_stats = {
                key: state['call_count']
                for key, state in self.key_states.items()
            }
            return {
                'total_calls': self.total_calls,
                'key_count': len(self.keys),
                'per_key_calls': key_stats
            }


# ============================================================================
# ä»£ç†åˆçº¦æ£€æµ‹æ¨¡å—
# ============================================================================

class ProxyDetector:
    """ä»£ç†åˆçº¦æ£€æµ‹å™¨ - æ£€æµ‹å¹¶è§£æå„ç§ä»£ç†æ¨¡å¼"""

    # EIP-1967: Logic contract (Implementation)
    EIP1967_LOGIC_SLOT = "0x360894a13ba1a3210667c828492db98dca3e2076cc3735a920a3ca505d382bbc"

    # EIP-1967: Beacon contract
    EIP1967_BEACON_SLOT = "0xa3f0ad74e5423aebfd80d3ef4346578335a9a72aeaee59ff6cb3582b35133d50"

    # EIP-1822: UUPS Proxiable
    EIP1822_LOGIC_SLOT = "0xc5f16f0fcc639fa48a6947836d9850f504798523bf8c9a3a87d5876cf622bcf7"

    # OpenZeppelin: Implementation slot (for older versions)
    OZ_IMPLEMENTATION_SLOT = "0x7050c9e0f4ca769c69bd3a8ef740bc37934f8e2c036e5a723fd8ee048ed3f8c3"

    def __init__(self, logger: logging.Logger):
        self.logger = logger

    def detect_proxy(self, address: str, chain: str) -> Optional[Dict[str, Any]]:
        """
        æ£€æµ‹åˆçº¦æ˜¯å¦ä¸ºä»£ç†,å¹¶è¿”å›å®ç°åˆçº¦åœ°å€

        Args:
            address: åˆçº¦åœ°å€
            chain: é“¾ç±»å‹

        Returns:
            ä»£ç†ä¿¡æ¯å­—å…¸æˆ–None: {
                'is_proxy': True,
                'proxy_type': 'EIP1967' | 'EIP1822' | 'Beacon' | 'Custom',
                'implementation': '0x...',
                'beacon': '0x...' (ä»…Beaconä»£ç†)
            }
        """
        if chain not in EXPLORER_APIS:
            return None

        api_config = EXPLORER_APIS[chain]
        rpc_url = api_config.get('rpc_url')

        if not rpc_url:
            self.logger.debug(f"  æœªé…ç½®RPC URLç”¨äºé“¾: {chain}, è·³è¿‡ä»£ç†æ£€æµ‹")
            return None

        try:
            # 1. æ£€æŸ¥ EIP-1967 Logic Slot
            impl_address = self._get_storage_at(rpc_url, address, self.EIP1967_LOGIC_SLOT)
            if impl_address and impl_address != "0x" + "0" * 40:
                return {
                    'is_proxy': True,
                    'proxy_type': 'EIP1967',
                    'implementation': impl_address
                }

            # 2. æ£€æŸ¥ EIP-1967 Beacon Slot
            beacon_address = self._get_storage_at(rpc_url, address, self.EIP1967_BEACON_SLOT)
            if beacon_address and beacon_address != "0x" + "0" * 40:
                # Beaconä»£ç†éœ€è¦å†ä»Beaconåˆçº¦ä¸­è·å–å®ç°åœ°å€
                impl_from_beacon = self._get_implementation_from_beacon(rpc_url, beacon_address)
                if impl_from_beacon:
                    return {
                        'is_proxy': True,
                        'proxy_type': 'Beacon',
                        'implementation': impl_from_beacon,
                        'beacon': beacon_address
                    }

            # 3. æ£€æŸ¥ EIP-1822 UUPS Slot
            impl_address = self._get_storage_at(rpc_url, address, self.EIP1822_LOGIC_SLOT)
            if impl_address and impl_address != "0x" + "0" * 40:
                return {
                    'is_proxy': True,
                    'proxy_type': 'EIP1822',
                    'implementation': impl_address
                }

            # 4. æ£€æŸ¥ OpenZeppelin æ—§ç‰ˆæœ¬ Slot
            impl_address = self._get_storage_at(rpc_url, address, self.OZ_IMPLEMENTATION_SLOT)
            if impl_address and impl_address != "0x" + "0" * 40:
                return {
                    'is_proxy': True,
                    'proxy_type': 'OpenZeppelin',
                    'implementation': impl_address
                }

            # æœªæ£€æµ‹åˆ°ä»£ç†
            return None

        except Exception as e:
            self.logger.debug(f"  ä»£ç†æ£€æµ‹å¤±è´¥: {e}")
            return None

    def _get_storage_at(self, rpc_url: str, address: str, slot: str) -> Optional[str]:
        """
        é€šè¿‡ eth_getStorageAt è¯»å–storage slot

        Returns:
            åœ°å€å­—ç¬¦ä¸²(0x...)æˆ–None
        """
        try:
            payload = {
                "jsonrpc": "2.0",
                "method": "eth_getStorageAt",
                "params": [address, slot, "latest"],
                "id": 1
            }

            response = requests.post(rpc_url, json=payload, timeout=10)
            response.raise_for_status()

            data = response.json()

            if 'result' not in data:
                return None

            storage_value = data['result']

            # storageå€¼æ˜¯32å­—èŠ‚,åœ°å€æ˜¯æœ€å20å­—èŠ‚
            if storage_value and len(storage_value) >= 42:
                # æå–æœ€å40ä¸ªåå…­è¿›åˆ¶å­—ç¬¦(20å­—èŠ‚)ä½œä¸ºåœ°å€
                address_hex = "0x" + storage_value[-40:]

                # æ£€æŸ¥æ˜¯å¦ä¸ºé›¶åœ°å€
                if address_hex == "0x" + "0" * 40:
                    return None

                return address_hex

            return None

        except Exception as e:
            self.logger.debug(f"  è¯»å–storageå¤±è´¥: {e}")
            return None

    def _get_implementation_from_beacon(self, rpc_url: str, beacon_address: str) -> Optional[str]:
        """
        ä»Beaconåˆçº¦ä¸­è·å–å®ç°åˆçº¦åœ°å€

        Beaconåˆçº¦é€šå¸¸æœ‰ implementation() å‡½æ•° (selector: 0x5c60da1b)
        """
        try:
            payload = {
                "jsonrpc": "2.0",
                "method": "eth_call",
                "params": [{
                    "to": beacon_address,
                    "data": "0x5c60da1b"  # implementation()
                }, "latest"],
                "id": 1
            }

            response = requests.post(rpc_url, json=payload, timeout=10)
            response.raise_for_status()

            data = response.json()

            if 'result' not in data:
                return None

            result = data['result']

            # è¿”å›å€¼æ˜¯32å­—èŠ‚,åœ°å€æ˜¯æœ€å20å­—èŠ‚
            if result and len(result) >= 42:
                address_hex = "0x" + result[-40:]

                if address_hex == "0x" + "0" * 40:
                    return None

                return address_hex

            return None

        except Exception as e:
            self.logger.debug(f"  ä»Beaconè·å–å®ç°åœ°å€å¤±è´¥: {e}")
            return None


# ============================================================================
# æºç ä¸‹è½½æ¨¡å—
# ============================================================================

class SourceDownloader:
    """æºç ä¸‹è½½å™¨ - ä»åŒºå—æµè§ˆå™¨ä¸‹è½½åˆçº¦æºç (æ”¯æŒå¤šKeyå¹¶å‘)"""

    def __init__(self, api_keys: Optional[Dict[str, Any]] = None):
        # ä½¿ç”¨ä¼ å…¥çš„API Keys,å¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        self.api_keys = api_keys or DEFAULT_API_KEYS
        self.logger = logging.getLogger(__name__ + '.SourceDownloader')

        # åˆå§‹åŒ–ä»£ç†æ£€æµ‹å™¨
        self.proxy_detector = ProxyDetector(self.logger)

        # ä¸ºæ¯ä¸ªkeyç±»å‹åˆ›å»ºKeyPool
        self.key_pools = {}
        for key_name, keys in self.api_keys.items():
            # å…¼å®¹æ—§æ ¼å¼(å•ä¸ªkeyå­—ç¬¦ä¸²)å’Œæ–°æ ¼å¼(keyåˆ—è¡¨)
            if isinstance(keys, str):
                keys = [keys]
            elif isinstance(keys, list):
                keys = keys
            else:
                self.logger.warning(f"æœªçŸ¥çš„keyæ ¼å¼: {key_name}, è·³è¿‡")
                continue

            # åˆ›å»ºè¯¥ç±»å‹çš„KeyPool
            self.key_pools[key_name] = APIKeyPool(keys, rate_limit=API_RATE_LIMIT)
            self.logger.info(f"ä¸º {key_name} åˆ›å»ºKeyæ± : {len(keys)} ä¸ªkey")

    def _get_key_pool(self, chain: str) -> Optional[APIKeyPool]:
        """æ ¹æ®é“¾åè·å–å¯¹åº”çš„KeyPool"""
        if chain not in EXPLORER_APIS:
            return None

        api_key_name = EXPLORER_APIS[chain].get('api_key_name', 'etherscan')
        key_pool = self.key_pools.get(api_key_name)

        if not key_pool:
            self.logger.warning(f"æœªé…ç½® {api_key_name} çš„API Key Pool")

        return key_pool

    def download_contract(self, address: ContractAddress, chain: str,
                         output_dir: Path, detect_proxy: bool = True,
                         _recursion_depth: int = 0) -> Tuple[bool, bool]:
        """
        ä¸‹è½½åˆçº¦æºç æˆ–å­—èŠ‚ç ,å¹¶è‡ªåŠ¨æ£€æµ‹å’Œä¸‹è½½ä»£ç†çš„å®ç°åˆçº¦

        Args:
            address: åˆçº¦åœ°å€
            chain: é“¾ç±»å‹
            output_dir: è¾“å‡ºç›®å½•
            detect_proxy: æ˜¯å¦æ£€æµ‹ä»£ç†(é»˜è®¤True)
            _recursion_depth: é€’å½’æ·±åº¦(å†…éƒ¨ä½¿ç”¨,é˜²æ­¢æ— é™é€’å½’)

        Returns:
            (æ˜¯å¦æˆåŠŸ, æ˜¯å¦ä»…å­—èŠ‚ç )
        """
        # é˜²æ­¢æ— é™é€’å½’(æœ€å¤š3å±‚: Proxy -> Beacon -> Implementation)
        MAX_RECURSION_DEPTH = 3
        if _recursion_depth >= MAX_RECURSION_DEPTH:
            self.logger.warning(f"  ä»£ç†é€’å½’æ·±åº¦è¶…è¿‡é™åˆ¶({MAX_RECURSION_DEPTH}),åœæ­¢æ£€æµ‹")
            return False, False

        if chain not in EXPLORER_APIS:
            self.logger.warning(f"ä¸æ”¯æŒçš„é“¾ç±»å‹: {chain}")
            return False, False

        api_config = EXPLORER_APIS[chain]

        # è·å–è¯¥é“¾å¯¹åº”çš„KeyPool
        key_pool = self._get_key_pool(chain)
        if not key_pool:
            self.logger.warning(f"  æœªé…ç½®APIå¯†é’¥ç”¨äºé“¾: {chain}")
            return False, False

        # åˆ›å»ºè¾“å‡ºç›®å½•
        contract_dir = output_dir / f"{address.address}_{address.name or 'Unknown'}"
        contract_dir.mkdir(parents=True, exist_ok=True)

        # ä¸‹è½½æºç (å¸¦é‡è¯•)
        success = False
        is_bytecode_only = False
        proxy_info = None

        for attempt in range(API_RETRY_TIMES):
            try:
                # ä»KeyPoolè·å–ä¸€ä¸ªAPI Key(è‡ªåŠ¨é™æµ)
                with key_pool.acquire_key() as api_key:
                    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨V1 API
                    use_v1 = api_config.get('use_v1', False)

                    # è°ƒç”¨API
                    source_code = self._fetch_source_code(
                        address.address,
                        api_config['api_url'],
                        api_config['chainid'],
                        api_key,
                        use_v1=use_v1
                    )

                    if not source_code:
                        # åˆçº¦æœªéªŒè¯,å°è¯•ä¸‹è½½å­—èŠ‚ç 
                        self.logger.info(f"  åˆçº¦æœªéªŒè¯,å°è¯•ä¸‹è½½å­—èŠ‚ç : {address.address[:10]}...")
                        bytecode_success = self._download_bytecode(address, chain, contract_dir)
                        if bytecode_success:
                            success = True
                            is_bytecode_only = True
                        break

                    # ä¿å­˜æºç 
                    self._save_contract_files(source_code, contract_dir)
                    success = True
                    is_bytecode_only = False

                self.logger.info(f"  âœ“ ä¸‹è½½æˆåŠŸ: {address.address[:10]}...")
                break

            except Exception as e:
                self.logger.warning(f"  ä¸‹è½½å¤±è´¥ (å°è¯• {attempt+1}/{API_RETRY_TIMES}): {e}")
                if attempt < API_RETRY_TIMES - 1:
                    time.sleep(API_RETRY_DELAY)

        # å¦‚æœä¸‹è½½æˆåŠŸä¸”å¯ç”¨ä»£ç†æ£€æµ‹,æ£€æŸ¥æ˜¯å¦ä¸ºä»£ç†åˆçº¦
        if success and detect_proxy:
            proxy_info = self.proxy_detector.detect_proxy(address.address, chain)

            if proxy_info and proxy_info.get('is_proxy'):
                impl_address = proxy_info.get('implementation')
                proxy_type = proxy_info.get('proxy_type')

                self.logger.info(f"  ğŸ”— æ£€æµ‹åˆ°{proxy_type}ä»£ç†,å®ç°åˆçº¦: {impl_address[:10]}...")

                # ä¿å­˜ä»£ç†ä¿¡æ¯åˆ°metadata
                self._save_proxy_info(contract_dir, proxy_info)

                # é€’å½’ä¸‹è½½å®ç°åˆçº¦
                impl_contract_addr = ContractAddress(
                    address=impl_address,
                    name=f"{address.name or 'Unknown'}_Implementation",
                    chain=address.chain,
                    source="proxy_implementation"
                )

                self.logger.info(f"  â†³ å¼€å§‹ä¸‹è½½å®ç°åˆçº¦...")
                impl_success, impl_is_bytecode = self.download_contract(
                    impl_contract_addr,
                    chain,
                    output_dir,
                    detect_proxy=True,  # å®ç°åˆçº¦ä¹Ÿå¯èƒ½æ˜¯ä»£ç†(å¦‚Beacon)
                    _recursion_depth=_recursion_depth + 1
                )

                if impl_success:
                    self.logger.info(f"  â†³ å®ç°åˆçº¦ä¸‹è½½æˆåŠŸ")
                else:
                    self.logger.warning(f"  â†³ å®ç°åˆçº¦ä¸‹è½½å¤±è´¥")

        return success, is_bytecode_only

    def _fetch_source_code(self, address: str, api_url: str, chainid: int, api_key: str, use_v1: bool = False) -> Optional[Dict]:
        """
        ä»APIè·å–æºç  (æ”¯æŒV1å’ŒV2 API)

        Returns:
            æºç ä¿¡æ¯å­—å…¸æˆ–None
        """
        if use_v1:
            # V1 APIä¸éœ€è¦chainidå‚æ•°
            params = {
                'module': 'contract',
                'action': 'getsourcecode',
                'address': address,
                'apikey': api_key
            }
        else:
            # V2 APIéœ€è¦chainidå‚æ•°
            params = {
                'chainid': chainid,
                'module': 'contract',
                'action': 'getsourcecode',
                'address': address,
                'apikey': api_key
            }

        response = requests.get(api_url, params=params, timeout=30)
        response.raise_for_status()

        data = response.json()

        if data['status'] != '1' or not data['result']:
            return None

        result = data['result'][0]

        # æ£€æŸ¥æ˜¯å¦å·²éªŒè¯
        if result['SourceCode'] == '':
            return None

        return result

    def _save_contract_files(self, source_code: Dict, output_dir: Path):
        """ä¿å­˜åˆçº¦æ–‡ä»¶"""

        # ä¿å­˜ä¸»æºç 
        source = source_code['SourceCode']

        # å¤„ç†å¤šæ–‡ä»¶åˆçº¦(JSONæ ¼å¼)
        if source.startswith('{{'):
            # ç§»é™¤å¤–å±‚çš„å¤§æ‹¬å·
            source = source[1:-1]
            sources = json.loads(source)

            # ä¿å­˜æ‰€æœ‰æºæ–‡ä»¶
            if 'sources' in sources:
                for file_path, file_data in sources['sources'].items():
                    file_output = output_dir / file_path.replace('/', '_')
                    with open(file_output, 'w', encoding='utf-8') as f:
                        f.write(file_data['content'])
        else:
            # å•æ–‡ä»¶åˆçº¦
            contract_file = output_dir / f"{source_code['ContractName']}.sol"
            with open(contract_file, 'w', encoding='utf-8') as f:
                f.write(source)

        # ä¿å­˜ABI
        if source_code['ABI'] != 'Contract source code not verified':
            abi_file = output_dir / 'abi.json'
            with open(abi_file, 'w', encoding='utf-8') as f:
                f.write(source_code['ABI'])

        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'contract_name': source_code['ContractName'],
            'compiler_version': source_code['CompilerVersion'],
            'optimization': source_code['OptimizationUsed'] == '1',
            'runs': source_code['Runs'],
            'constructor_arguments': source_code.get('ConstructorArguments', ''),
            'evm_version': source_code.get('EVMVersion', ''),
            'library': source_code.get('Library', ''),
            'license_type': source_code.get('LicenseType', ''),
            'proxy': source_code.get('Proxy', '0') == '1',
            'implementation': source_code.get('Implementation', '')
        }

        metadata_file = output_dir / 'metadata.json'
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2)

    def _save_proxy_info(self, output_dir: Path, proxy_info: Dict[str, Any]):
        """
        ä¿å­˜ä»£ç†ä¿¡æ¯åˆ°metadata.json

        Args:
            output_dir: åˆçº¦è¾“å‡ºç›®å½•
            proxy_info: ä»£ç†ä¿¡æ¯å­—å…¸
        """
        try:
            metadata_file = output_dir / 'metadata.json'

            # è¯»å–ç°æœ‰metadata
            if metadata_file.exists():
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
            else:
                metadata = {}

            # æ·»åŠ ä»£ç†ä¿¡æ¯
            metadata['proxy_detected'] = True
            metadata['proxy_type'] = proxy_info.get('proxy_type')
            metadata['implementation_address'] = proxy_info.get('implementation')

            if 'beacon' in proxy_info:
                metadata['beacon_address'] = proxy_info.get('beacon')

            # ä¿å­˜æ›´æ–°åçš„metadata
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2)

        except Exception as e:
            self.logger.warning(f"  ä¿å­˜ä»£ç†ä¿¡æ¯å¤±è´¥: {e}")

    def _download_bytecode(self, address: ContractAddress, chain: str, output_dir: Path) -> bool:
        """
        ä¸‹è½½æœªéªŒè¯åˆçº¦çš„å­—èŠ‚ç (é€šè¿‡RPC)

        Args:
            address: åˆçº¦åœ°å€
            chain: é“¾ç±»å‹
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if chain not in EXPLORER_APIS:
            self.logger.warning(f"  ä¸æ”¯æŒçš„é“¾ç±»å‹: {chain}")
            return False

        api_config = EXPLORER_APIS[chain]
        rpc_url = api_config.get('rpc_url')

        if not rpc_url:
            self.logger.warning(f"  æœªé…ç½®RPC URLç”¨äºé“¾: {chain}")
            return False

        try:
            # é€šè¿‡eth_getCodeè·å–å­—èŠ‚ç 
            payload = {
                "jsonrpc": "2.0",
                "method": "eth_getCode",
                "params": [address.address, "latest"],
                "id": 1
            }

            response = requests.post(rpc_url, json=payload, timeout=30)
            response.raise_for_status()

            data = response.json()

            if 'result' not in data:
                self.logger.warning(f"  RPCè¿”å›æ— æ•ˆæ•°æ®: {address.address}")
                return False

            bytecode = data['result']

            # æ£€æŸ¥æ˜¯å¦ä¸ºç©º(0xæˆ–0x0)
            if not bytecode or bytecode == '0x' or bytecode == '0x0':
                self.logger.warning(f"  åœ°å€ä¸æ˜¯åˆçº¦æˆ–å·²è‡ªæ¯: {address.address}")
                return False

            # ä¿å­˜å­—èŠ‚ç 
            bytecode_file = output_dir / 'bytecode.hex'
            with open(bytecode_file, 'w', encoding='utf-8') as f:
                f.write(bytecode)

            # è®¡ç®—å­—èŠ‚ç å¤§å°
            bytecode_size = (len(bytecode) - 2) // 2  # å‡å»0xå‰ç¼€,é™¤ä»¥2å¾—åˆ°å­—èŠ‚æ•°

            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                'address': address.address,
                'chain': chain,
                'verified': False,
                'bytecode_size': bytecode_size,
                'bytecode_file': 'bytecode.hex',
                'note': 'åˆçº¦æœªåœ¨åŒºå—æµè§ˆå™¨ä¸ŠéªŒè¯,ä»…åŒ…å«å­—èŠ‚ç '
            }

            metadata_file = output_dir / 'metadata.json'
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2)

            self.logger.info(f"  âœ“ å­—èŠ‚ç ä¸‹è½½æˆåŠŸ: {address.address[:10]}... (å¤§å°: {bytecode_size} bytes)")
            return True

        except Exception as e:
            self.logger.error(f"  ä¸‹è½½å­—èŠ‚ç å¤±è´¥: {e}")
            return False

    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰KeyPoolçš„ç»Ÿè®¡ä¿¡æ¯"""
        all_stats = {}
        for key_name, key_pool in self.key_pools.items():
            all_stats[key_name] = key_pool.get_stats()
        return all_stats


# ============================================================================
# ä¸»æ§åˆ¶å™¨
# ============================================================================

class ContractExtractor:
    """ä¸»æ§åˆ¶å™¨ - åè°ƒå„ä¸ªæ¨¡å—"""

    def __init__(self, test_dir: Path, output_dir: Path,
                 api_keys: Optional[Dict[str, str]] = None,
                 diff_enabled: bool = False,
                 force_overwrite: bool = False):
        self.test_dir = test_dir
        self.output_dir = output_dir

        # ä½¿ç”¨ä¼ å…¥çš„api_keysæˆ–é»˜è®¤é…ç½®
        self.api_keys = api_keys or DEFAULT_API_KEYS

        # åˆå§‹åŒ–å„æ¨¡å—
        self.static_analyzer = StaticAnalyzer()
        self.dynamic_analyzer = DynamicAnalyzer()
        self.source_downloader = SourceDownloader(self.api_keys)

        # diff è¡Œä¸ºæ§åˆ¶
        self.diff_enabled = diff_enabled
        self.force_overwrite = force_overwrite
        self.diff_results: List[Dict[str, Any]] = []

        # ç»Ÿè®¡ä¿¡æ¯
        self.summary = ExecutionSummary()

        # æ—¥å¿—
        self.logger = logging.getLogger(__name__ + '.ContractExtractor')

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # é”™è¯¯æ—¥å¿—æ–‡ä»¶
        self.error_log = self.output_dir / 'error.log'
        self.unverified_file = self.output_dir / 'unverified.json'
        self.summary_file = self.output_dir / 'summary.json'

    def extract_all(self, date_filters: Optional[List[str]] = None):
        """
        æå–æ‰€æœ‰è„šæœ¬çš„åˆçº¦

        Args:
            date_filters: æ—¥æœŸè¿‡æ»¤å™¨åˆ—è¡¨,å¦‚ ["2025-08","2025-09"]
        """
        self.logger.info("=" * 80)
        self.logger.info("å¼€å§‹æå–DeFiæ”»å‡»åˆçº¦")
        self.logger.info("=" * 80)

        # æŸ¥æ‰¾æ‰€æœ‰æµ‹è¯•è„šæœ¬
        scripts = self._find_all_scripts(date_filters)
        self.summary.total_scripts = len(scripts)

        self.logger.info(f"æ‰¾åˆ° {len(scripts)} ä¸ªæµ‹è¯•è„šæœ¬")

        # å¤„ç†æ¯ä¸ªè„šæœ¬
        unverified_contracts = []

        for i, script in enumerate(scripts, 1):
            self.logger.info(f"\n[{i}/{len(scripts)}] å¤„ç†: {script.date_dir}/{script.name}")

            try:
                success = self._process_script(script, unverified_contracts)
                if success:
                    self.summary.successful_scripts += 1
                else:
                    self.summary.failed_scripts += 1
            except Exception as e:
                error_msg = f"å¤„ç†è„šæœ¬å¤±è´¥ {script.name}: {e}"
                self.logger.error(error_msg)
                self.summary.errors.append(error_msg)
                self.summary.failed_scripts += 1
                self._log_error(error_msg)

        # ä¿å­˜æœªéªŒè¯åˆçº¦åˆ—è¡¨
        if unverified_contracts:
            with open(self.unverified_file, 'w') as f:
                json.dump(unverified_contracts, f, indent=2)

        # ä¿å­˜æ‰§è¡Œæ‘˜è¦
        self._save_summary()

        # æ‰“å°ç»Ÿè®¡
        self._print_summary()
        self._print_diff_report()

    def _find_all_scripts(self, date_filters: Optional[List[str]] = None) -> List[ExploitScript]:
        """æŸ¥æ‰¾æ‰€æœ‰æµ‹è¯•è„šæœ¬"""
        scripts = []

        # éå†æ‰€æœ‰æ—¥æœŸç›®å½•
        for date_dir in sorted(self.test_dir.iterdir()):
            if not date_dir.is_dir():
                continue

            # åŒ¹é… YYYY-MM æ ¼å¼
            if not re.match(r'\d{4}-\d{2}', date_dir.name):
                continue

            # åº”ç”¨è¿‡æ»¤å™¨
            if date_filters and not any(date_dir.name.startswith(f) for f in date_filters):
                continue

            # æŸ¥æ‰¾è¯¥ç›®å½•ä¸‹çš„æ‰€æœ‰.solæ–‡ä»¶
            for sol_file in date_dir.glob('*.sol'):
                script = ExploitScript(
                    file_path=sol_file,
                    name=sol_file.stem,
                    date_dir=date_dir.name
                )
                scripts.append(script)

        return scripts

    def _process_script(self, script: ExploitScript,
                       unverified_contracts: List[Dict]) -> bool:
        """
        å¤„ç†å•ä¸ªè„šæœ¬

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        # 1. é™æ€åˆ†æ
        static_addresses, chain = self.static_analyzer.analyze_script(script)
        script.chain = chain or script.chain

        # 2. åŠ¨æ€åˆ†æ
        dynamic_addresses = []
        if self.dynamic_analyzer:
            dynamic_addresses = self.dynamic_analyzer.analyze_script(script)

        # 3. åˆå¹¶åœ°å€
        all_addresses = self._merge_addresses(static_addresses, dynamic_addresses, script.chain)

        if not all_addresses:
            self.logger.warning("  æœªæå–åˆ°ä»»ä½•åœ°å€")
            return False

        self.logger.info(f"  å…±æå–åˆ° {len(all_addresses)} ä¸ªå”¯ä¸€åœ°å€")
        self.summary.total_addresses += len(all_addresses)

        # 4. åˆ›å»ºè¾“å‡ºç›®å½•
        script_output_dir = self.output_dir / script.date_dir / script.name
        script_output_dir.mkdir(parents=True, exist_ok=True)

        existing_addresses = self._load_existing_addresses(script_output_dir)
        if self.diff_enabled:
            diff_info = self._calculate_diff(existing_addresses or [], all_addresses)
            self._record_diff(script, script_output_dir, diff_info, existing_addresses is not None)

            if existing_addresses and not self.force_overwrite:
                self.logger.info("  diffæ¨¡å¼ï¼šå·²å±•ç¤ºå·®å¼‚ï¼Œå¦‚éœ€è¦†ç›–è¯·å¢åŠ  --force")
                return True

        # 5. ä¿å­˜åœ°å€åˆ—è¡¨
        self._save_addresses(all_addresses, script_output_dir / 'addresses.json')

        # 6. ä¸‹è½½æºç 
        if script.chain:
            self._download_sources(all_addresses, script.chain,
                                  script_output_dir, unverified_contracts)
        else:
            self.logger.warning("  æœªè¯†åˆ«é“¾ç±»å‹,è·³è¿‡æºç ä¸‹è½½")

        return True

    def _merge_addresses(self, static: List[ContractAddress],
                        dynamic: List[ContractAddress],
                        chain: Optional[str]) -> List[ContractAddress]:
        """åˆå¹¶é™æ€å’ŒåŠ¨æ€åœ°å€"""
        # ä½¿ç”¨å­—å…¸å»é‡,ä¿ç•™æ›´å¤šä¿¡æ¯
        merged = {}

        for addr in static + dynamic:
            key = addr.address.lower()
            if key in merged:
                # åˆå¹¶ä¿¡æ¯
                existing = merged[key]
                if not existing.name and addr.name:
                    existing.name = addr.name
                if not existing.chain and addr.chain:
                    existing.chain = addr.chain
            else:
                if not addr.chain and chain:
                    addr.chain = chain
                merged[key] = addr

        return list(merged.values())

    def _save_addresses(self, addresses: List[ContractAddress], output_file: Path):
        """ä¿å­˜åœ°å€åˆ—è¡¨"""
        data = [asdict(addr) for addr in addresses]
        with open(output_file, 'w') as f:
            json.dump(data, f, indent=2)

    def _download_sources(self, addresses: List[ContractAddress],
                         chain: str, output_dir: Path,
                         unverified_contracts: List[Dict]):
        """ä¸‹è½½æ‰€æœ‰åœ°å€çš„æºç (å¹¶å‘ä¸‹è½½)"""
        if not addresses:
            return

        # è·å–è¯¥é“¾çš„KeyPool,ç¡®å®šå¹¶å‘çº¿ç¨‹æ•°
        key_pool = self.source_downloader._get_key_pool(chain)
        if not key_pool:
            self.logger.warning(f"  æœªé…ç½®API Key,è·³è¿‡æºç ä¸‹è½½")
            return

        # ä½¿ç”¨keyæ•°é‡ä½œä¸ºå¹¶å‘çº¿ç¨‹æ•°(æ¯ä¸ªçº¿ç¨‹ä½¿ç”¨ä¸€ä¸ªkey)
        max_workers = len(key_pool.keys)
        self.logger.info(f"  å¼€å§‹ä¸‹è½½æºç  (é“¾: {chain}, å¹¶å‘æ•°: {max_workers})")

        # ä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤å…±äº«æ•°æ®
        stats_lock = threading.Lock()

        def download_one(addr: ContractAddress) -> bool:
            """ä¸‹è½½å•ä¸ªåˆçº¦(ä¾›çº¿ç¨‹æ± ä½¿ç”¨)"""
            success, is_bytecode_only = self.source_downloader.download_contract(addr, chain, output_dir)

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯(éœ€è¦åŠ é”)
            with stats_lock:
                if success:
                    if is_bytecode_only:
                        self.summary.bytecode_only_contracts += 1
                    else:
                        self.summary.verified_contracts += 1
                else:
                    self.summary.unverified_contracts += 1
                    unverified_contracts.append({
                        'address': addr.address,
                        'chain': chain,
                        'name': addr.name
                    })

            return success

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘ä¸‹è½½
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä¸‹è½½ä»»åŠ¡
            futures = {
                executor.submit(download_one, addr): addr
                for addr in addresses
            }

            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for future in as_completed(futures):
                addr = futures[future]
                try:
                    success = future.result()
                except Exception as e:
                    self.logger.error(f"  ä¸‹è½½ {addr.address} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                    with stats_lock:
                        self.summary.unverified_contracts += 1

    def _load_existing_addresses(self, script_output_dir: Path) -> Optional[List[Dict]]:
        """è¯»å–ç°æœ‰çš„ addresses.json"""
        addresses_file = script_output_dir / 'addresses.json'
        if not addresses_file.exists():
            return None
        try:
            with open(addresses_file, 'r') as f:
                data = json.load(f)
            if isinstance(data, list):
                return data
            self.logger.warning(f"  addresses.json æ ¼å¼å¼‚å¸¸: {addresses_file}")
            return None
        except Exception as e:
            self.logger.warning(f"  è¯»å–ç°æœ‰ addresses.json å¤±è´¥: {e}")
            return None

    def _calculate_diff(self, old_list: List[Dict], new_list: List[ContractAddress]) -> Dict[str, Any]:
        """è®¡ç®—æ–°æ—§æ•°æ®çš„å·®å¼‚"""
        new_dicts = [asdict(addr) for addr in new_list]

        old_map = self._addresses_to_map(old_list)
        new_map = self._addresses_to_map(new_dicts)

        added = []
        for entry in new_dicts:
            address = entry.get('address')
            if not isinstance(address, str):
                continue
            if address.lower() not in old_map:
                added.append(entry)

        removed = []
        for entry in old_list:
            address = entry.get('address')
            if not isinstance(address, str):
                continue
            if address.lower() not in new_map:
                removed.append(entry)

        changed = []
        for addr_lower, old_entry in old_map.items():
            if addr_lower not in new_map:
                continue
            new_entry = new_map[addr_lower]
            field_changes = {}
            for field in ('name', 'chain', 'source', 'context'):
                if old_entry.get(field) != new_entry.get(field):
                    field_changes[field] = {
                        'old': old_entry.get(field),
                        'new': new_entry.get(field)
                    }
            if field_changes:
                changed.append({
                    'address': new_entry.get('address', old_entry.get('address')),
                    'changes': field_changes
                })

        return {
            'added': added,
            'removed': removed,
            'changed': changed
        }

    def _addresses_to_map(self, entries: List[Dict]) -> Dict[str, Dict]:
        mapping: Dict[str, Dict] = {}
        for entry in entries:
            address = entry.get('address')
            if not isinstance(address, str):
                continue
            mapping[address.lower()] = entry
        return mapping

    def _record_diff(self, script: ExploitScript, script_output_dir: Path,
                     diff_info: Dict[str, Any], had_baseline: bool):
        """è¾“å‡ºå¹¶è®°å½•å•ä¸ªè„šæœ¬çš„å·®å¼‚"""
        added = diff_info.get('added', [])
        removed = diff_info.get('removed', [])
        changed = diff_info.get('changed', [])

        summary = f"+{len(added)} -{len(removed)} ~{len(changed)}"
        if not had_baseline:
            summary += " (é¦–æ¬¡ç”ŸæˆåŸºçº¿)"
        self.logger.info(f"  diff ç»“æœ: {summary}")

        if added:
            for entry in added:
                self.logger.info(f"    + {entry.get('address')} ({entry.get('name') or '-'})")
        if removed:
            for entry in removed:
                self.logger.info(f"    - {entry.get('address')} ({entry.get('name') or '-'})")
        if changed:
            for entry in changed:
                changes_desc = ", ".join(
                    f"{field}: {detail['old']} -> {detail['new']}"
                    for field, detail in entry['changes'].items()
                )
                self.logger.info(f"    ~ {entry['address']}: {changes_desc}")

        self.diff_results.append({
            'script': script.name,
            'date_dir': script.date_dir,
            'path': str(script_output_dir),
            'added': added,
            'removed': removed,
            'changed': changed
        })

    def _print_diff_report(self):
        """æ‰“å° diff æ±‡æ€»æŠ¥å‘Š"""
        if not self.diff_enabled:
            return

        self.logger.info("\n" + "=" * 80)
        self.logger.info("Diff æŠ¥å‘Š")
        self.logger.info("=" * 80)

        if not self.diff_results:
            self.logger.info("æœªç”Ÿæˆ diff ç»“æœ")
            return

        for item in self.diff_results:
            stats = f"+{len(item['added'])} -{len(item['removed'])} ~{len(item['changed'])}"
            self.logger.info(f"{item['date_dir']}/{item['script']}: {stats}")
            if item['added']:
                for entry in item['added']:
                    self.logger.info(f"  + {entry.get('address')} ({entry.get('name') or '-'})")
            if item['removed']:
                for entry in item['removed']:
                    self.logger.info(f"  - {entry.get('address')} ({entry.get('name') or '-'})")
            if item['changed']:
                for entry in item['changed']:
                    changes_desc = ", ".join(
                        f"{field}: {detail['old']} -> {detail['new']}"
                        for field, detail in entry['changes'].items()
                    )
                    self.logger.info(f"  ~ {entry['address']}: {changes_desc}")


    def _log_error(self, message: str):
        """è®°å½•é”™è¯¯åˆ°æ—¥å¿—æ–‡ä»¶"""
        with open(self.error_log, 'a') as f:
            f.write(f"{time.strftime('%Y-%m-%d %H:%M:%S')} - {message}\n")

    def _save_summary(self):
        """ä¿å­˜æ‰§è¡Œæ‘˜è¦"""
        # è·å–KeyPoolç»Ÿè®¡ä¿¡æ¯
        key_stats = self.source_downloader.get_stats()
        total_api_calls = sum(stats['total_calls'] for stats in key_stats.values())

        self.summary.api_calls = total_api_calls

        # ä¿å­˜å®Œæ•´æ‘˜è¦(åŒ…å«Keyç»Ÿè®¡)
        summary_with_stats = asdict(self.summary)
        summary_with_stats['key_pool_stats'] = key_stats

        with open(self.summary_file, 'w') as f:
            json.dump(summary_with_stats, f, indent=2)

    def _print_summary(self):
        """æ‰“å°æ‰§è¡Œæ‘˜è¦"""
        self.logger.info("\n" + "=" * 80)
        self.logger.info("æ‰§è¡Œæ‘˜è¦")
        self.logger.info("=" * 80)
        self.logger.info(f"æ€»è„šæœ¬æ•°:        {self.summary.total_scripts}")
        self.logger.info(f"æˆåŠŸ:            {self.summary.successful_scripts}")
        self.logger.info(f"å¤±è´¥:            {self.summary.failed_scripts}")
        self.logger.info(f"æ€»åœ°å€æ•°:        {self.summary.total_addresses}")
        self.logger.info(f"å·²éªŒè¯åˆçº¦:      {self.summary.verified_contracts}")
        self.logger.info(f"æœªéªŒè¯åˆçº¦:      {self.summary.unverified_contracts}")
        self.logger.info(f"  â””â”€ ä»…å­—èŠ‚ç :   {self.summary.bytecode_only_contracts}")
        self.logger.info(f"APIè°ƒç”¨æ¬¡æ•°:     {self.summary.api_calls}")

        # æ‰“å°KeyPoolç»Ÿè®¡
        key_stats = self.source_downloader.get_stats()
        if key_stats:
            self.logger.info(f"\nAPI Keyå¹¶å‘ç»Ÿè®¡:")
            for key_name, stats in key_stats.items():
                self.logger.info(f"  {key_name}:")
                self.logger.info(f"    Keyæ•°é‡: {stats['key_count']}")
                self.logger.info(f"    æ€»è°ƒç”¨: {stats['total_calls']}")
                # æ˜¾ç¤ºæ¯ä¸ªkeyçš„è´Ÿè½½å‡è¡¡æƒ…å†µ
                per_key = stats['per_key_calls']
                if per_key:
                    call_counts = list(per_key.values())
                    self.logger.info(f"    è´Ÿè½½å‡è¡¡: æœ€å°={min(call_counts)}, æœ€å¤§={max(call_counts)}, å¹³å‡={sum(call_counts)/len(call_counts):.1f}")

        self.logger.info(f"\nè¾“å‡ºç›®å½•:        {self.output_dir}")
        if self.summary.errors:
            self.logger.info(f"é”™è¯¯æ•°:          {len(self.summary.errors)}")
            self.logger.info(f"é”™è¯¯æ—¥å¿—:        {self.error_log}")
        self.logger.info("=" * 80)


# ============================================================================
# å‘½ä»¤è¡Œæ¥å£
# ============================================================================

def main():
    import argparse

    parser = argparse.ArgumentParser(
        description='DeFiæ”»å‡»åˆçº¦æºç æå–å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å¤„ç†2025-08ç›®å½•
  python extract_contracts.py --filter 2025-08

  # å¤„ç†æ‰€æœ‰è„šæœ¬
  python extract_contracts.py

  # ä½¿ç”¨API Key
  python extract_contracts.py --api-key YOUR_API_KEY

  # åŒæ—¶å¤„ç†å¤šä¸ªç›®å½•
  python extract_contracts.py --filter 2025-08 --filter 2025-09

  # åªåšé™æ€åˆ†æ(ä¸è¿è¡Œæµ‹è¯•)
  python extract_contracts.py --static-only

  # å¯¹æ¯”å·²æœ‰ç»“æœä½†ä¸è¦†ç›–
  python extract_contracts.py --filter 2024-01 --diff

  # å¯¹æ¯”å¹¶è¦†ç›–
  python extract_contracts.py --filter 2024-01 --diff --force
        """
    )

    parser.add_argument(
        '--test-dir',
        type=Path,
        default=DEFAULT_TEST_DIR,
        help='æµ‹è¯•è„šæœ¬ç›®å½• (é»˜è®¤: é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„src/test)'
    )

    parser.add_argument(
        '--output-dir',
        type=Path,
        default=DEFAULT_OUTPUT_DIR,
        help='è¾“å‡ºç›®å½• (é»˜è®¤: é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„extracted_contracts)'
    )

    parser.add_argument(
        '--filter',
        dest='filters',
        action='append',
        help='æ—¥æœŸè¿‡æ»¤å™¨,å¯é‡å¤ä½¿ç”¨æˆ–ç”¨é€—å·åˆ†éš”å¤šä¸ªå€¼,å¦‚ "2025-08,2025-09"(é»˜è®¤: å¤„ç†æ‰€æœ‰)'
    )

    parser.add_argument(
        '--api-key',
        type=str,
        help='(å¯é€‰) è¦†ç›–é»˜è®¤çš„API Keyé…ç½®'
    )

    parser.add_argument(
        '--static-only',
        action='store_true',
        help='åªåšé™æ€åˆ†æ,ä¸è¿è¡Œforge test'
    )

    parser.add_argument(
        '--diff',
        action='store_true',
        help='æ¯”è¾ƒæ–°æ—§ç»“æœï¼Œè¾“å‡ºå·®å¼‚ï¼ˆé»˜è®¤ä¸è¦†ç›–å·²æœ‰æ–‡ä»¶ï¼‰'
    )

    parser.add_argument(
        '--force',
        action='store_true',
        help='ä¸ --diff æ­é…æ—¶å¼ºåˆ¶è¦†ç›–è¾“å‡ºç›®å½•ï¼Œä»ä¼šæ‰“å°å·®å¼‚'
    )

    parser.add_argument(
        '--debug',
        action='store_true',
        help='å¯ç”¨è°ƒè¯•æ—¥å¿—'
    )

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)

    # API Keyså·²ç¡¬ç¼–ç åœ¨DEFAULT_API_KEYSä¸­
    # å¦‚æœç”¨æˆ·æä¾›äº†api-keyå‚æ•°,å¯ä»¥è¦†ç›–Etherscan key
    api_keys = DEFAULT_API_KEYS.copy()
    if args.api_key:
        api_keys['etherscan'] = args.api_key
        logger.info(f"ä½¿ç”¨è‡ªå®šä¹‰Etherscan API Key")
    else:
        logger.info("ä½¿ç”¨ç¡¬ç¼–ç çš„API Keysé…ç½®")

    # åˆ›å»ºæå–å™¨
    extractor = ContractExtractor(
        test_dir=args.test_dir,
        output_dir=args.output_dir,
        api_keys=api_keys,
        diff_enabled=args.diff,
        force_overwrite=args.force
    )

    # å¦‚æœåªåšé™æ€åˆ†æ,ç¦ç”¨åŠ¨æ€åˆ†æå™¨
    if args.static_only:
        extractor.dynamic_analyzer = None
        logger.info("åªæ‰§è¡Œé™æ€åˆ†ææ¨¡å¼")

    # è§£ææ—¥æœŸè¿‡æ»¤å™¨
    date_filters = None
    if args.filters:
        parsed_filters = []
        for value in args.filters:
            for item in value.split(','):
                item = item.strip()
                if item:
                    parsed_filters.append(item)
        if parsed_filters:
            date_filters = sorted(set(parsed_filters))

    # æ‰§è¡Œæå–
    try:
        extractor.extract_all(date_filters=date_filters)
    except KeyboardInterrupt:
        logger.info("\n\nç”¨æˆ·ä¸­æ–­,æ­£åœ¨ä¿å­˜è¿›åº¦...")
        extractor._save_summary()
        logger.info("è¿›åº¦å·²ä¿å­˜")


if __name__ == '__main__':
    main()
